# 서버 튜닝

https://x.com/darjeelingt/status/1766261553492644226?s=20

고수는 상황을 확인한다.  
하수는 시스템(분산처리/MSA) 로 답한다.

# 1. 먼저 메트릭을 붙이고 기록한다.

## 1.1 인프라 메트릭

기본적인 인프라관련 내용

- CPU 사용율 ( 코어별도 기록하면 좋다. )
- 메모리 사용률
- IO 사용 ( 디스크, 소켓등 )
- 네트워크 소켓 사용관련 ( ss 에 나오는 packet loss, resend, retrieve )
- io 사용따라 다른데 여튼 분리 가능하면 죄다 기록

## 1.2 플랫폼 + 언어 메트릭

- JVM 의 경우 gc 관련 내용 full gc count 등
- 닷넷이나 다른 언어는 모르는데 찾아보고 기록
- 비동기가 주요인 js 의 경우 blocking ? time 이런것
- 쓰레드수나 뭐 이런것?
- AWS 등의 플랫폼에 언혀갈 경우 관련 메트릭

## 1.3. 어플리케이션 메트릭 1

여기서부터 애매한데 왜냐면 너무 많기 떄문에..

- 일단 API 써버라면 호출에 걸리는 시간
- 디비라면 라운드 트립 타임, 그럼 호출이 여러번이면!? ㅠㅠ
- 라운드 트립인경우도 있고 db pool 의 경우 pool 을 얻는데 걸리는 시간을 기록해야하는 경우도 있음

## 1.3. 어플리케이션 메트릭 2

- 경우에 따라서 호출에 들어간 함수 갯수나 가져온 갯수를 기록하는 경우도 있음.
- 비동기의 경우 특히 중간에 버퍼링 사이즈나 작업 사이즈를 계산해야하기 때문에 관련 사이즈까지도 기록해두면 좋음.
- 컨슈머의 경우도 마찬가지로 요청받은 사이즈등 전부 기록

## 1.4 기록방법 1 ?

지금은 메트릭의 경우 프로메테우스 프로토콜을 태워서 아무데나 메트릭 디비에 때려박고 그라파나로 보는게 가장 평범한듯  
물론 각 플랫폼에 따라서 나뉨  
여튼 중요한것은 메트릭을 전부다 기록하고  
이걸 조합해서 보거나 노티를 받는 방안이 필요함

## 1.4 기록방법 2 ?

로그에서 메트릭을 뽑는것도 가능하고 필요함.  
예를 들면 로그를 남겼을때 거기서 메트릭을 뽑을수 있음.  
이럴 경우에 이걸 어플리케이션 레벨로도 할수 있고  
아니면 vector 나 뭐 이런 툴들을 쓸수도 있음  
혹은 정말 로그를 저장한후에 로그스토리지에서 뽑을수도있음.

## 1.5. 대쉬보드

그래서 일단 메트릭 + 로그를 남겼다면 이걸 통합해서 볼수 있어야함.  
지금은 그라파나 같은걸 주로 쓰는데 물론 플랫폼마다 툴이 다 있음.  
주요한것은 메트릭에 대한 쿼리를 적절하게 남길수 있어야하고  
어그리게이션등이 가능한 플랫폼이면 좋음.  
노티까지 가능해야함.

# 2. 배포 1

그래서 일단 모든 메트릭을 붙이고 배포를 해야함.  
그리고 배포시에 revision 에 대한 기록이 필요함.  
설정의 revision 도 따로 기록가능하면 좋음.  
revision 과 metric 은 어느정도 연관을 지을수 있어야 나중에 메트릭이 튈경우에 그걸로 git 에서 역으로 추적이 가능

# 2. 배포 2

배포도 여러가지 가능이 필요한데 일단 이건 별도로 하고..  
여러가지 파라미터를 추가 가능해서 메모리, 코어, 뭐 환경변수를 조절해야함. 그래야 나중에 튜닝이 가능함. 주로 JVM 의 경우 gc type, 메모리양 뭐 이런애들이 있는데 다른 언어의 경우도 있는지 확인이 필요

# 3. 초기 서비스 1

일단 초기 서비스의 경우 3대로 시작 ( 고가용성 및 세션 기능 확인 때문 ) 물론 디비는 최소 리플리케이션  
메트릭을 보고 스펙을 올리거나 써버의 갯수를 올리는게 필요함  
근데 메트릭은 여러가지 레벨에서 확인이 가능하므로 전부다 확인이 필요함.

# 3. 초기 서비스 2

API 별 레이턴시를 목표로 잡을수도 있는데  
이경우에 내부에서 처리하는 시간을 목표로 한다.  
왜냐면 클라이언트가 가져가는것은 외부의 성능지표라서 나는 관여가 불가능함  
그리고 나의 경우 처음엔 노드나 인스턴트당 CPU 30%가 될때까지 쪼이거나 늘림

# 3. 초기 서비스 3

그리고 캐쉬가 가능한 asset 은 cdn 을 태우거나 기타등등 가급적 분리하는 것이 좋음.

물론 초기 서비스를 하기전에 테스트를 하는게 좋은데..

# 4. 성능 테스트 1

프로덕션에 돌리기전에 jmeter, nGrinder 나 k6 등 각 회사에 맞는 툴로 성능 테스트를 해보는게 좋음. 당연히 이때도 메트릭을 쎄워야함.  
e2e 로 테스트를 해도 되고 단순히 유저수만 늘려서 테스트를 해도 되는데 기본적으로 격리된 환경에서 테스트가 가능하도록 해서

# 4. 성능 테스트 2

동접얼마에 요청이 몇개면 CPU 나 뭐 이런걸 얼마나 쓴다.  
얼마나 요청하면 타임아웃이 난다  
메모리를 얼마나 쓰고 디비가 얼마나 쓰이고  
이런걸 적당히 문서화해서 지표로 삼을필요가 있음.  
이걸 보통 프로덕션 투입전에 해야함.

# 5. 성능 튜닝 1

기본적으로는 파라미터를 잘 설정해두었다면  
JVM 에 대한 튜닝은 문제가 없음.  
이때 노드가 여러개라면 1대에만 적용해서 메트릭의 차이를 보는것도 좋음

# 5. 성능 튜닝 2

그런데 이제 튜닝에 대한 근거가 필요함.  
이부분은 언어마다 다른데 자바의 경우 jmap 이나 jvm 옵션등으로 얼마나 메모리나 쓰레드가 있는지 알수 있고  
뭐 워낙에 툴이 많은데 다른 언어는 모르겠음.  
힙덤프를 떠서 그거 기준으로 어플리케이션 내부에 대한 튜닝이 필요함.

# 5. 성능 튜닝 3

자료구조나 비지니스 로직에 대한 점검이 필요할수도 있음.  
예를 들면 통짜로 천개를 처리하는걸 100개로 쪼개서 10번하는게 나을수도 있음. ( GC 나 메모리 사용 )  
혹은 너무 비동기를 잘게 잘라서 발라서 처리큐가 밀려서 안되는걸수도있음.

# 5. 성능 튜닝 4

이때를 위해서 메트릭, 로깅이 필요하고 지속적으로 비교해가면서 튜닝해야함.  
그리고 이때 사이즈와 노드 갯수도 어느정도 결정되는편..

# 6. 결론 1

이것이 일반론인데 사실 모든것은 돈문제임  
인력이 없으면 메트릭을 쎼우지도 못하고  
그냥 돈버는 메트릭에 의존하는게 나을수도 있고  
모니터링하고 튜닝하는 것보다 그냥 높은 스펙의 장비를 아무 생각없이 쓰는게 나을수가 있음.  
결론은 돈임

예를 들면 1억짜리 DevOps 엔지니어가 줄여주어야하는 비용은 최소 연간 2~3억임.  
물론 배포나 다른 프로세스를 개선함으로도 가능하지만  
사람들은 돈에 민감해서 바로 현금으로 환급되길 바람. ( 저평가함)  
그러므로 고급 엔지니어를 구하는것보다 그냥 Pod 를 늘리고 버티는 방안도 맞음.
